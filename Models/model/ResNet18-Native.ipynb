{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c4ef87-9de4-410a-b22e-091ec7f194e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a901a6c-c136-4ae0-b5b9-efc6f90d9513",
   "metadata": {},
   "source": [
    "## Komponen Utama ResNet18 Native yang Dibuat\n",
    "\n",
    "1. **Conv2D** ‚úÖ \n",
    "   - Konvolusi 2D dengan parameter kernel, stride, padding  \n",
    "   - Forward & Backward propagation  \n",
    "   - Mendukung input multi-channel (RGB)\n",
    "\n",
    "2. **BatchNorm2D** ‚úÖ \n",
    "   - Normalisasi batch untuk stabilisasi dan percepatan training  \n",
    "   - Forward & Backward propagation  \n",
    "\n",
    "3. **ReLU (Rectified Linear Unit)** ‚úÖ \n",
    "   - Fungsi aktivasi non-linear $ \\max(0, x) $  \n",
    "   - Forward & Backward propagation  \n",
    "\n",
    "4. **MaxPool2D** ‚úÖ \n",
    "   - Pooling maksimal dengan kernel dan stride tertentu  \n",
    "   - Mengurangi dimensi spasial (height & width)  \n",
    "   - Forward & Backward propagation  \n",
    "\n",
    "5. **GlobalAveragePooling2D (GAP)** ‚úÖ \n",
    "   - Pooling rata-rata global untuk mereduksi spatial dimension ke 1 nilai per channel  \n",
    "   - Forward & Backward propagation  \n",
    "\n",
    "6. **ResidualBlock (BasicBlock)** ‚úÖ \n",
    "   - Blok residual dengan dua layer Conv2D + BatchNorm + ReLU  \n",
    "   - Skip connection (identity atau projection)  \n",
    "   - Forward & Backward propagation  \n",
    "\n",
    "7. **Dense (Fully Connected Layer)** ‚úÖ \n",
    "   - Layer linear terakhir untuk klasifikasi  \n",
    "   - Forward & Backward propagation  \n",
    "\n",
    "8. **Dropout** ‚úÖ \n",
    "   - Regularisasi dengan menonaktifkan neuron secara acak saat training  \n",
    "   - Forward & Backward propagation  \n",
    "\n",
    "9. **Loss Function: SoftmaxCrossEntropyLoss** ‚úÖ \n",
    "   - Fungsi loss klasifikasi multi-kelas  \n",
    "   - Forward & Backward propagation  \n",
    "\n",
    "10. **Optimizer (contoh: Adam, SGD)** ‚úÖ \n",
    "    - Update parameter berdasarkan gradien dan algoritma optimasi  \n",
    "    - Mempunyai persamaan matematis terkait update bobot\n",
    "    \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d506788-8688-4708-aac9-66b43f9418d1",
   "metadata": {},
   "source": [
    "### Conv2D - Persamaan Matematis\n",
    "\n",
    "Untuk setiap filter $ f $ dan posisi $ (i, j) $, output dihitung dengan:\n",
    "\n",
    "$$\n",
    "y_{i,j,f} = \\sum_{c=1}^{C_{in}} \\sum_{m=1}^{k} \\sum_{n=1}^{k} W_{f,c,m,n} \\cdot x_{i+m, j+n, c} + b_f\n",
    "$$\n",
    "\n",
    "Keterangan:\n",
    "Keterangan:\n",
    "\n",
    "- $C_{in}$: jumlah channel input (contoh: 3 untuk RGB)  \n",
    "- $W$: bobot filter (kernel) bentuk $(F, C_{in}, k, k)$  \n",
    "- $b_f$: bias untuk filter ke-$f$  \n",
    "- $x$: input citra dengan ukuran height $\\times$ width  \n",
    "\n",
    "#### üîÅ Backward: Gradien terhadap parameter dan input\n",
    "\n",
    "- Gradien terhadap bobot $ W $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{f,c,m,n}} = \\sum_{i,j,b} \\frac{\\partial L}{\\partial y_{i,j,f}} \\cdot x_{i+m,j+n,c}\n",
    "$$\n",
    "\n",
    "- Gradien terhadap input $ x $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i,j,c}} = \\sum_{f} \\sum_{m,n} \\frac{\\partial L}{\\partial y_{i-m,j-n,f}} \\cdot W_{f,c,m,n}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5673b3f-072e-4c18-a4c7-303e31ea998a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        \"\"\"\n",
    "        in_channels: jumlah channel input (misal: 3 untuk RGB)\n",
    "        out_channels: jumlah filter\n",
    "        kernel_size: ukuran kernel (biasanya 3)\n",
    "        stride: pergeseran filter\n",
    "        padding: jumlah padding di sekeliling input\n",
    "        \"\"\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Weight: (out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.W = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * np.sqrt(2. / (in_channels * kernel_size * kernel_size))\n",
    "        self.b = np.zeros(out_channels)\n",
    "\n",
    "        # Gradien\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input dengan shape (batch_size, height, width, in_channels)\n",
    "        output: (batch_size, new_height, new_width, out_channels)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        batch_size, h_in, w_in, _ = x.shape\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        p = self.padding\n",
    "\n",
    "        # Padding\n",
    "        x_padded = np.pad(x, ((0, 0), (p, p), (p, p), (0, 0)), mode='constant')\n",
    "        self.x_padded = x_padded\n",
    "\n",
    "        h_out = (h_in + 2*p - k) // s + 1\n",
    "        w_out = (w_in + 2*p - k) // s + 1\n",
    "        out = np.zeros((batch_size, h_out, w_out, self.out_channels))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h_out):\n",
    "                for j in range(w_out):\n",
    "                    for f in range(self.out_channels):\n",
    "                        h_start = i * s\n",
    "                        w_start = j * s\n",
    "                        region = x_padded[b, h_start:h_start+k, w_start:w_start+k, :]\n",
    "                        out[b, i, j, f] = np.sum(region * self.W[f]) + self.b[f]\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output, lr):\n",
    "        \"\"\"\n",
    "        grad_output: gradien dari layer berikutnya, shape (batch, h_out, w_out, out_channels)\n",
    "        \"\"\"\n",
    "        batch_size, h_out, w_out, _ = grad_output.shape\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        p = self.padding\n",
    "\n",
    "        self.dW.fill(0)\n",
    "        self.db.fill(0)\n",
    "        dx_padded = np.zeros_like(self.x_padded)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h_out):\n",
    "                for j in range(w_out):\n",
    "                    for f in range(self.out_channels):\n",
    "                        h_start = i * s\n",
    "                        w_start = j * s\n",
    "                        region = self.x_padded[b, h_start:h_start+k, w_start:w_start+k, :]\n",
    "\n",
    "                        self.dW[f] += region * grad_output[b, i, j, f]\n",
    "                        self.db[f] += grad_output[b, i, j, f]\n",
    "                        dx_padded[b, h_start:h_start+k, w_start:w_start+k, :] += self.W[f] * grad_output[b, i, j, f]\n",
    "\n",
    "        # Update weight & bias\n",
    "        self.W -= lr * self.dW\n",
    "        self.b -= lr * self.db\n",
    "\n",
    "        # Remove padding\n",
    "        if p > 0:\n",
    "            dx = dx_padded[:, p:-p, p:-p, :]\n",
    "        else:\n",
    "            dx = dx_padded\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2575c2-32fb-4f72-a00f-6c9ca4b1373f",
   "metadata": {},
   "source": [
    "### BatchNorm2D - Persamaan Matematis\n",
    "\n",
    "\n",
    "Mean dan variansi:\n",
    "\n",
    "$$\n",
    "\\mu_c = \\frac{1}{N} \\sum_{i=1}^{N} x_{i,c}, \\quad\n",
    "\\sigma_c^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_{i,c} - \\mu_c)^2\n",
    "$$\n",
    "\n",
    "Normalisasi:\n",
    "\n",
    "$$\n",
    "\\hat{x}_{i,c} = \\frac{x_{i,c} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Skalasi dan translasi:\n",
    "\n",
    "$$\n",
    "y_{i,c} = \\gamma_c \\hat{x}_{i,c} + \\beta_c\n",
    "$$\n",
    "\n",
    "#### üîÅ Backward:\n",
    "\n",
    "- Gradien terhadap skala dan bias:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\gamma_c} = \\sum_i \\frac{\\partial L}{\\partial y_{i,c}} \\cdot \\hat{x}_{i,c}\n",
    "\\quad ; \\quad\n",
    "\\frac{\\partial L}{\\partial \\beta_c} = \\sum_i \\frac{\\partial L}{\\partial y_{i,c}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a99c33f-4bff-43b8-b115-7a92f5e9cfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchNorm2D:\n",
    "    def __init__(self, num_features, momentum=0.9, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        num_features: jumlah channel fitur (biasanya sama dengan output channel dari Conv2D)\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Parameter trainable\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "\n",
    "        # Untuk inference\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        x: input dengan shape (batch, height, width, channels)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        if training:\n",
    "            self.mean = x.mean(axis=(0, 1, 2))\n",
    "            self.var = x.var(axis=(0, 1, 2))\n",
    "\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "\n",
    "            self.x_norm = (x - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "        else:\n",
    "            self.x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "\n",
    "        out = self.gamma * self.x_norm + self.beta\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output, lr):\n",
    "        \"\"\"\n",
    "        grad_output: gradien dari layer berikutnya, shape (batch, height, width, channels)\n",
    "        \"\"\"\n",
    "        N, H, W, C = grad_output.shape\n",
    "\n",
    "        dx_norm = grad_output * self.gamma\n",
    "        dvar = np.sum(dx_norm * (self.x - self.mean) * -0.5 * ((self.var + self.epsilon) ** -1.5), axis=(0, 1, 2))\n",
    "        dmean = np.sum(dx_norm * -1 / np.sqrt(self.var + self.epsilon), axis=(0, 1, 2)) + \\\n",
    "                dvar * np.sum(-2 * (self.x - self.mean), axis=(0, 1, 2)) / (N * H * W)\n",
    "\n",
    "        dx = dx_norm / np.sqrt(self.var + self.epsilon) + \\\n",
    "             dvar * 2 * (self.x - self.mean) / (N * H * W) + \\\n",
    "             dmean / (N * H * W)\n",
    "\n",
    "        self.dgamma = np.sum(grad_output * self.x_norm, axis=(0, 1, 2))\n",
    "        self.dbeta = np.sum(grad_output, axis=(0, 1, 2))\n",
    "\n",
    "        # Update gamma dan beta\n",
    "        self.gamma -= lr * self.dgamma\n",
    "        self.beta -= lr * self.dbeta\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0fe53-0251-41c7-9e7e-5eeaf4c14bc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ReLU (Fungsi Aktivasi) - Persamaan Matematis\n",
    "\n",
    "Fungsi aktivasi:\n",
    "\n",
    "$$\n",
    "y = \\max(0, x)\n",
    "$$\n",
    "\n",
    "#### üîÅ Backward:\n",
    "Turunan (backpropagation):\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "\\begin{cases}\n",
    "1, & \\text{jika } x > 0 \\\\\n",
    "0, & \\text{jika } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7303e4d9-cf41-421b-abf0-3c40bd736cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None  # simpan posisi > 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input (batch, height, width, channels)\n",
    "        \"\"\"\n",
    "        self.mask = (x > 0)\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        grad_output: gradien dari layer selanjutnya\n",
    "        \"\"\"\n",
    "        return grad_output * self.mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb62fcac-c355-40b4-8925-d848ddb6493f",
   "metadata": {},
   "source": [
    "### MaxPooling 2D - Persamaan Matematis\n",
    "\n",
    "Misalkan input feature map $ x $ dengan ukuran $(H \\times W)$ kernel pooling berukuran $ k \\times k $, dan stride $ s $.\n",
    "\n",
    "Output feature map $ y $ dengan ukuran $(H_{out} \\times W_{out})$ dihitung dengan:\n",
    "\n",
    "$$\n",
    "y(i,j) = \\max_{(m,n) \\in R(i,j)} x(m,n)\n",
    "$$\n",
    "\n",
    "di mana\n",
    "\n",
    "$$\n",
    "R(i,j) = \\{ (m,n) \\mid m = s \\cdot i + a, \\quad n = s \\cdot j + b, \\quad 0 \\leq a,b < k \\}\n",
    "$$\n",
    "\n",
    "dan ukuran output adalah\n",
    "\n",
    "$$\n",
    "H_{out} = \\left\\lfloor \\frac{H - k}{s} \\right\\rfloor + 1, \\quad W_{out} = \\left\\lfloor \\frac{W - k}{s} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "#### üîÅ Backward:\n",
    "Gradien terhadap input $ x $ hanya diteruskan ke posisi maksimal dalam setiap area pooling, yaitu\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x(m,n)} = \n",
    "\\begin{cases}\n",
    "\\frac{\\partial L}{\\partial y(i,j)}, & \\text{jika } (m,n) = \\arg\\max_{(a,b) \\in R(i,j)} x(a,b) \\\\\n",
    "0, & \\text{lainnya}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "466d56b7-8ef0-4a5a-a5b4-c076537bd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: numpy array with shape (batch_size, channels, height, width)\n",
    "        \"\"\"\n",
    "        self.input = x\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "\n",
    "        out_height = (height - k) // s + 1\n",
    "        out_width = (width - k) // s + 1\n",
    "        self.output = np.zeros((batch_size, channels, out_height, out_width))\n",
    "        self.argmax = np.zeros_like(self.output, dtype=int)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        h_start = i * s\n",
    "                        w_start = j * s\n",
    "                        window = x[b, c, h_start:h_start+k, w_start:w_start+k]\n",
    "                        self.output[b, c, i, j] = np.max(window)\n",
    "                        self.argmax[b, c, i, j] = np.argmax(window)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        grad_output: numpy array with shape same as self.output\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = self.input.shape\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        grad_input = np.zeros_like(self.input)\n",
    "\n",
    "        out_height, out_width = grad_output.shape[2], grad_output.shape[3]\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        h_start = i * s\n",
    "                        w_start = j * s\n",
    "                        window = self.input[b, c, h_start:h_start+k, w_start:w_start+k]\n",
    "                        max_idx = self.argmax[b, c, i, j]\n",
    "                        max_pos = np.unravel_index(max_idx, window.shape)\n",
    "                        grad_input[b, c, h_start:h_start+k, w_start:w_start+k][max_pos] += grad_output[b, c, i, j]\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54eb43-64d9-4699-b351-adf054fd52b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Global Average Pooling (GAP)\n",
    "\n",
    "Diberikan input $ x \\in \\mathbb{R}^{B \\times C \\times H \\times W} $, maka output GAP:\n",
    "\n",
    "$$\n",
    "y_{b,c} = \\frac{1}{H \\cdot W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{b,c,i,j}\n",
    "$$\n",
    "\n",
    "Hasil: Tensor ukuran $ B \\times C $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4e345b-0149-4729-a215-9216c7355930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GlobalAveragePooling2D:\n",
    "    def forward(self, x):\n",
    "        self.input = x  # shape: (B, C, H, W)\n",
    "        return np.mean(x, axis=(2, 3))  # shape: (B, C)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        B, C, H, W = self.input.shape\n",
    "        grad_input = grad_output[:, :, None, None] * np.ones((B, C, H, W)) / (H * W)\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95787702-2145-441c-80ba-bf644017855d",
   "metadata": {},
   "source": [
    "### Dense Layer / Fully Connected\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "y = xW + b\n",
    "$$\n",
    "\n",
    "Dengan:\n",
    "- $ x \\in \\mathbb{R}^{B \\times D} $ \n",
    "- $ W \\in \\mathbb{R}^{D \\times K} $, $ b \\in \\mathbb{R}^{K} $\n",
    "- $ y \\in \\mathbb{R}^{B \\times K} $, jumlah kelas = $ K $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "608bfc15-95e7-4b01-a86a-eee0ce44c066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
    "        self.b = np.zeros(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x  # shape: (B, D)\n",
    "        return x @ self.W + self.b  # shape: (B, K)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.grad_W = self.input.T @ grad_output  # shape: (D, K)\n",
    "        self.grad_b = np.sum(grad_output, axis=0)  # shape: (K,)\n",
    "        grad_input = grad_output @ self.W.T  # shape: (B, D)\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9629f40-b7a0-4742-8d19-d2a48281585f",
   "metadata": {},
   "source": [
    "### Residual Block (ResNet BasicBlock)\n",
    "\n",
    "Blok residual menghitung:\n",
    "\n",
    "$$\n",
    "F(x) = \\text{ReLU}(\\text{BN}_2(\\text{Conv}_2(\\text{ReLU}(\\text{BN}_1(\\text{Conv}_1(x))))))\n",
    "$$\n",
    "\n",
    "Lalu hasil akhir:\n",
    "\n",
    "$$\n",
    "y = F(x) + x\n",
    "$$\n",
    "\n",
    "Jika terjadi perubahan dimensi (misal karena stride > 1 atau jumlah channel berubah), maka shortcut diubah:\n",
    "\n",
    "$$\n",
    "x' = \\text{BN}_{\\text{proj}}(\\text{Conv}_{1 \\times 1}(x))\n",
    "$$\n",
    "\n",
    "Sehingga:\n",
    "\n",
    "$$\n",
    "y = F(x) + x'\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "839e61d8-1fbd-4191-bd82-6790087ea10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualBlock:\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        self.conv1 = Conv2D(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = BatchNorm2D(out_channels)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "        self.conv2 = Conv2D(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = BatchNorm2D(out_channels)\n",
    "\n",
    "        self.use_projection = (stride != 1 or in_channels != out_channels)\n",
    "        if self.use_projection:\n",
    "            self.shortcut_conv = Conv2D(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "            self.shortcut_bn = BatchNorm2D(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out)\n",
    "        out = self.relu.forward(out)\n",
    "\n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out)\n",
    "\n",
    "        if self.use_projection:\n",
    "            shortcut = self.shortcut_conv.forward(x)\n",
    "            shortcut = self.shortcut_bn.forward(shortcut)\n",
    "        else:\n",
    "            shortcut = x  # asumsi ukuran sama\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_output = self.relu.backward(grad_output)\n",
    "\n",
    "        grad_shortcut = grad_output.copy()\n",
    "        grad_main = grad_output.copy()\n",
    "\n",
    "        # Backward main path\n",
    "        grad_main = self.bn2.backward(grad_main)\n",
    "        grad_main = self.conv2.backward(grad_main)\n",
    "        grad_main = self.relu.backward(grad_main)\n",
    "        grad_main = self.bn1.backward(grad_main)\n",
    "        grad_main = self.conv1.backward(grad_main)\n",
    "\n",
    "        # Backward shortcut\n",
    "        if self.use_projection:\n",
    "            grad_shortcut = self.shortcut_bn.backward(grad_shortcut)\n",
    "            grad_shortcut = self.shortcut_conv.backward(grad_shortcut)\n",
    "\n",
    "        # Combine grads\n",
    "        grad_input = grad_main + grad_shortcut\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0d84a-f5c5-4548-89ac-cf0fd4c9d477",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Diberikan output logits $ z \\in \\mathbb{R}^K $ maka softmax-nya:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd2b54-0535-41e0-b683-4a6941dca27a",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss\n",
    "\n",
    "Untuk ground truth one-hot $ y $, dan prediksi softmax $ \\hat{y} $:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4b2ef9f-43aa-4053-8399-1b03d4f87893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropyLoss:\n",
    "    def forward(self, logits, labels):\n",
    "        # logits: (B, K), labels: (B, K) one-hot\n",
    "        self.logits = logits\n",
    "        self.labels = labels\n",
    "\n",
    "        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # stabilize\n",
    "        self.probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        batch_size = logits.shape[0]\n",
    "        loss = -np.sum(labels * np.log(self.probs + 1e-12)) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        # Derivatif loss terhadap logits\n",
    "        return (self.probs - self.labels) / self.labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba8cef-e70e-4642-af4d-b7f29f17c46c",
   "metadata": {},
   "source": [
    "# üß† Dropout\n",
    "\n",
    "### üîπ Forward Pass saat Training\n",
    "\n",
    "Diberikan input $ x \\in \\mathbb{R}^n $, kita sampling mask $ r \\in \\{0,1\\}^n $ dari distribusi Bernoulli:\n",
    "\n",
    "$$\n",
    "r_i \\sim \\text{Bernoulli}(1 - p)\n",
    "$$\n",
    "\n",
    "Kemudian output $ y $ dihitung sebagai:\n",
    "\n",
    "$$\n",
    "y_i = \\frac{x_i \\cdot r_i}{1 - p}\n",
    "$$\n",
    "\n",
    "> Di sini $ p $ adalah probabilitas dropout (misal: 0.5).\n",
    "\n",
    "### üîπ Forward Pass saat Inferensi\n",
    "\n",
    "$$\n",
    "y_i = x_i\n",
    "$$\n",
    "\n",
    "Tidak ada dropout saat inferensi ‚Äî semua neuron aktif.\n",
    "\n",
    "#### üîÅ Backward:\n",
    "Gradien terhadap input saat backpropagation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial y_i} \\cdot \\frac{r_i}{1 - p}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "952a8aa0-81f5-48d0-ba01-69c5901a83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.training = True  # Default mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.mask = (np.random.rand(*x.shape) > self.dropout_rate).astype(np.float32)\n",
    "            return x * self.mask / (1.0 - self.dropout_rate)  # Scale to keep expectation\n",
    "        else:\n",
    "            return x  # No dropout during evaluation\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.training:\n",
    "            return grad_output * self.mask / (1.0 - self.dropout_rate)\n",
    "        else:\n",
    "            return grad_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6afd8-a6e7-40ee-a4c5-d374f2706be8",
   "metadata": {},
   "source": [
    "# üß† Arsitektur ResNet-18 (Residual Network)\n",
    "\n",
    "**ResNet-18** adalah model Convolutional Neural Network (CNN) dengan 18 lapisan berparameter yang menggunakan **skip connection** (shortcut/identity/projection) untuk mengatasi masalah **degradasi gradien** saat melatih jaringan yang sangat dalam.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Struktur ResNet-18\n",
    "\n",
    "### üîπ Layer Awal:\n",
    "- **Conv2D**: 7√ó7, 64 filter, stride=2, padding=3  \n",
    "- **BatchNorm2D**  \n",
    "- **ReLU**  \n",
    "- **MaxPooling2D**: 3√ó3, stride=2  \n",
    "\n",
    "### üî∏ Residual Blocks:\n",
    "\n",
    "#### üß© Stage 1:\n",
    "- 2 √ó BasicBlock(64, 64)\n",
    "\n",
    "#### üß© Stage 2:\n",
    "- 1 √ó BasicBlock(64 ‚Üí 128, stride=2, projection)\n",
    "- 1 √ó BasicBlock(128, 128)\n",
    "\n",
    "#### üß© Stage 3:\n",
    "- 1 √ó BasicBlock(128 ‚Üí 256, stride=2, projection)\n",
    "- 1 √ó BasicBlock(256, 256)\n",
    "\n",
    "#### üß© Stage 4:\n",
    "- 1 √ó BasicBlock(256 ‚Üí 512, stride=2, projection)\n",
    "- 1 √ó BasicBlock(512, 512)\n",
    "\n",
    "---\n",
    "\n",
    "### üîö Akhir:\n",
    "- **Global Average Pooling (GAP)**  \n",
    "- **Dropout** (p=0.5, saat training)  \n",
    "- **Dense Layer**: 512 ‚Üí `num_classes`  \n",
    "- **Softmax Cross Entropy Loss**\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Total Layer Berparameter:\n",
    "- 1 √ó Conv awal  \n",
    "- 16 √ó Conv layer di dalam 8 BasicBlock  \n",
    "- 1 √ó Dense (FC)  \n",
    "‚Üí **Total: 18 weight layers**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Tambahan Komponen:\n",
    "- üîÑ **Skip Connection**:  \n",
    "  - Identity mapping (dimensi sama)  \n",
    "  - 1√ó1 projection (untuk penyesuaian channel atau stride)  \n",
    "- üõë **Dropout**: digunakan sebelum Dense untuk regularisasi  \n",
    "- üß† **Global Average Pooling (GAP)**: mengurangi dimensi spasial tanpa parameter  \n",
    "- ‚úñÔ∏è **Flatten**: tidak diperlukan karena output GAP sudah berupa vektor 1D\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Ringkasan Keunggulan:\n",
    "- Memungkinkan pelatihan jaringan dalam dengan kestabilan\n",
    "- Mengurangi overfitting dengan **Dropout**\n",
    "- Tidak memerlukan Flatten karena adanya **GAP**\n",
    "- Kompatibel dengan **data RGB (3 channel)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f730e1a5-6e91-4a40-8772-a7206b10ff6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet18:\n",
    "    def __init__(self, input_channels=3, num_classes=3):\n",
    "        self.conv1 = Conv2D(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = BatchNorm2D(64)\n",
    "        self.relu = ReLU()\n",
    "        self.maxpool = MaxPool2D(kernel_size=3, stride=2, padding=1)  # padding added for 'same' output size\n",
    "        self.dropout = Dropout(p=0.5)\n",
    "        \n",
    "        # Residual stages (2 blocks per layer as ResNet18)\n",
    "        self.layer1 = [ResidualBlock(64, 64) for _ in range(2)]\n",
    "        self.layer2 = [ResidualBlock(64, 128, stride=2, use_projection=True), ResidualBlock(128, 128)]\n",
    "        self.layer3 = [ResidualBlock(128, 256, stride=2, use_projection=True), ResidualBlock(256, 256)]\n",
    "        self.layer4 = [ResidualBlock(256, 512, stride=2, use_projection=True), ResidualBlock(512, 512)]\n",
    "\n",
    "        self.global_pool = GlobalAveragePooling2D()\n",
    "        self.fc = Dense(512, num_classes)\n",
    "        self.loss_fn = SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, y=None, training=True):\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.bn1.forward(x, training=training)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.maxpool.forward(x)\n",
    "\n",
    "        for block in self.layer1: x = block.forward(x, training=training)\n",
    "        for block in self.layer2: x = block.forward(x, training=training)\n",
    "        for block in self.layer3: x = block.forward(x, training=training)\n",
    "        for block in self.layer4: x = block.forward(x, training=training)\n",
    "\n",
    "        x = self.global_pool.forward(x)\n",
    "\n",
    "        if training:\n",
    "            x = self.dropout.forward(x)\n",
    "\n",
    "        logits = self.fc.forward(x)\n",
    "\n",
    "        if y is not None:\n",
    "            loss = self.loss_fn.forward(logits, y)\n",
    "            return logits, loss\n",
    "        return logits\n",
    "\n",
    "    def backward(self):\n",
    "        grad = self.loss_fn.backward()\n",
    "        grad = self.fc.backward(grad)\n",
    "\n",
    "        # Dropout backward (only if used in forward)\n",
    "        grad = self.dropout.backward(grad)\n",
    "\n",
    "        grad = self.global_pool.backward(grad)\n",
    "\n",
    "        for block in reversed(self.layer4): grad = block.backward(grad)\n",
    "        for block in reversed(self.layer3): grad = block.backward(grad)\n",
    "        for block in reversed(self.layer2): grad = block.backward(grad)\n",
    "        for block in reversed(self.layer1): grad = block.backward(grad)\n",
    "\n",
    "        grad = self.maxpool.backward(grad)\n",
    "        grad = self.relu.backward(grad)\n",
    "        grad = self.bn1.backward(grad)\n",
    "        grad = self.conv1.backward(grad)\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef0e99-fcc1-47db-bedd-1c7f0a6a647e",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Optimasi SGD melakukan update parameter $ \\theta $ dengan aturan:\n",
    "\n",
    "$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_{\\theta} J(\\theta_t)\n",
    "$\n",
    "\n",
    "- $ \\theta_t $ : parameter pada iterasi ke-$ t $  \n",
    "- $ \\eta $ : learning rate  \n",
    "- $ \\nabla_{\\theta} J(\\theta_t) $ : gradien fungsi loss terhadap parameter $ \\theta $ pada iterasi ke-$ t $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "112d8e18-3928-4c77-80ee-33e7486156ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        self.parameters = parameters  # list of params (weights & biases)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            if hasattr(param, 'grad'):\n",
    "                param -= self.lr * param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbdfd0-c1d6-4edc-bb37-f9ce795e8910",
   "metadata": {},
   "source": [
    "### Update Bobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "954a7785-f447-431d-91b4-bc95024aaf0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gather_params(model):\n",
    "    params = []\n",
    "    # contoh untuk Conv2D\n",
    "    params.append(model.conv1.W)\n",
    "    params.append(model.conv1.b)\n",
    "    # loop semua ResidualBlock dan ambil paramnya\n",
    "    for block in model.layer1 + model.layer2 + model.layer3 + model.layer4:\n",
    "        params.extend([block.conv1.W, block.conv1.b,\n",
    "                       block.conv2.W, block.conv2.b])\n",
    "        if block.use_projection:\n",
    "            params.extend([block.shortcut_conv.W, block.shortcut_conv.b])\n",
    "    # Dense layer\n",
    "    params.append(model.fc.W)\n",
    "    params.append(model.fc.b)\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "499dea7f-86e0-4f3b-8882-15558d706f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            logits, loss = model.forward(x_batch, y_batch)\n",
    "            model.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss * x_batch.shape[0]\n",
    "            preds = np.argmax(logits, axis=1)\n",
    "            labels = np.argmax(y_batch, axis=1)\n",
    "            total_correct += np.sum(preds == labels)\n",
    "            total_samples += x_batch.shape[0]\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss/total_samples:.4f}, Accuracy={total_correct/total_samples:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9c421-1e9e-4048-8934-aff694f18389",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam menggabungkan momentum dan adaptasi learning rate menggunakan estimasi momen pertama dan kedua:\n",
    "\n",
    "- Hitung gradien saat ini:\n",
    "$$\n",
    "g_t = \\nabla_{\\theta} J(\\theta_t)\n",
    "$$\n",
    "\n",
    "- Update momen pertama (mean gradien):\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "\n",
    "- Update momen kedua (mean kuadrat gradien):\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "\n",
    "- Koreksi bias momen:\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "- Update parameter:\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "- Di mana:\n",
    "  - $ \\beta_1, \\beta_2 $ adalah hyperparameter decay rate (biasanya 0.9 dan 0.999)  \n",
    "  - $ \\epsilon $ adalah nilai kecil untuk menghindari pembagian nol (biasanya $10^{-8}$)  \n",
    "  - $ \\eta $ adalah learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d8f50cb-7f85-4dcf-8085-1e179be423e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, parameters, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Initialize moment estimates\n",
    "        self.m = [np.zeros_like(p) for p in self.parameters]\n",
    "        self.v = [np.zeros_like(p) for p in self.parameters]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, param in enumerate(self.parameters):\n",
    "            if hasattr(param, 'grad'):\n",
    "                g = param.grad\n",
    "                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g ** 2)\n",
    "\n",
    "                m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4e06e8d-5d2b-40b8-8761-2fa65e171f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        logits, loss = model.forward(x_batch, y_batch)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        labels = np.argmax(y_batch, axis=1)\n",
    "\n",
    "        total_loss += loss * x_batch.shape[0]\n",
    "        total_correct += np.sum(preds == labels)\n",
    "        total_samples += x_batch.shape[0]\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Validation: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "    return avg_loss, accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
